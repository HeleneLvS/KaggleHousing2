---
title: "R Notebook for the Housepricing Comp"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
##clean out the space
rm(list=ls())
options(scipen=999)

#Chunk options
options(scipen=999) ###No scientific notifications
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message =  FALSE)
```

```{r admin, include = FALSE}

##What do we need
library(tidyr)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(cowplot) # plot different ggplots next to each other
library(DataExplorer) # Best EDA package by far
library(corrplot)
library(pscl) # For Pseudo R squareds on GLMS
library(caTools) #sample
library(pscl)
library(GGally) #corrs
library(MASS) # Stepwise
library(data.table) # for melt
#Not used lets remove later
#library(stats)

#Where will we work
#getwd()
setwd("C:/Users/helen/Documents/Kaggle/HousePrice/KaggleHousing")

```

The Data explorer package is great for a first stab at unknown data. First things first;

-   The dependent variable - the thing we want to predict - is continuous --\> regression problem
    -   GLM: Gamma with Log, Inverse or Identity link
    -   Regression Tree
    -   Random Forrest Reg Tree
-   Not normally distributed --\> important to match the algo to the distribution transformation
    -   Gool ol' OLS with a transformation?
-   Loads of missing data to remedy
    -   Which means we should add the test and train sets together - to save effort when scoring & validating

### Raw Combined Data File

```{r EDA}

train<-read.csv("train.csv")

train['Type']<-'train_set'

test<-read.csv("test.csv")
test['SalePrice'] <- NA
test['Type']<-'test_set'

data<-rbind(train,test)

#create_report(train)
#create_report(data)
plot_intro(data)
plot_missing(data)
```

Start by filling in the missing data and running some correlation plots to understand the multicollinearity;

-   By completing the NA values we drop from 5.8% missing observations to 0.33%

-   The remaining two significantly missing values are for

    -   LotFrontage and

    -   Garage built

-   The latter's missing value shows there is no Garage on the premises while the LotFrontage might just show that the property is not on a municipal street. However this may be an incorrect assumption so an alternative is to build a model on LotFrontage as a factor of LotConfig and LotArea. On closer inspection imputing missings with 0's ruins a pretty nice linear correlation between SalePrice and LotFrontage so lets start with the model. There is a separate Impute notebook for details on this. Garage built will be imputed with current year + 1 to represent the future.

```{r CleanupandFilled}
    #Fill in the NA's where they represent 'None Present' with 'None' 
data_f<-data

data_f$BsmtQual<- case_when(
          (is.na(data$BsmtQual) == TRUE & data$TotalBsmtSF >= 0) ~ data$BsmtCond,
          (is.na(data$BsmtQual) == TRUE & data$TotalBsmtSF == 0) ~ "None",
          TRUE~data$BsmtQual
          )
data_f$BsmtCond<- case_when(
          (is.na(data$BsmtCond) == TRUE & data$TotalBsmtSF > 0) ~ "TA",
          (is.na(data$BsmtCond) == TRUE & data$TotalBsmtSF == 0) ~ "None",
          TRUE~data$BsmtCond
          )
data_f$BsmtExposure<-case_when(
          (is.na(data$BsmtExposure) == TRUE & data$TotalBsmtSF > 1000) ~ "Av",
          (is.na(data$BsmtExposure) == TRUE & data$TotalBsmtSF > 0 
                    & data$TotalBsmtSF <= 1000) ~ "No",
          (is.na(data$BsmtExposure) == TRUE & data$TotalBsmtSF == 0) ~ "None",
          TRUE~data$BsmtExposure
          )

data_f$BsmtFinType2<-case_when(
          (is.na(data$BsmtFinType2) == TRUE & data$TotalBsmtSF > 0)~ "ALQ",
          (is.na(data$BsmtFinType2) == TRUE & data$TotalBsmtSF == 0) ~ "None",
          TRUE~data$BsmtFinType2
          )

data_f<-data_f %>%
      replace_na(list(Alley = "None", BsmtQual ="None", BsmtExposure = "None",
                      BsmtCond= "None", BsmtFinType1="None", BsmtFinType2="None",  
                      FireplaceQu = "None", 
                      GarageType = "None",GarageFinish = "None",
                      GarageQual="None", GarageCond="None", 
                      PoolQC="None", Fence = "None", GarageQual="None", 
                      GarageCond = "None",  MiscFeature = "None",
                      MasVnrType = "None"))
data_f<-data_f %>% 
      replace_na(list(GarageCars = 0,
                      GarageArea = 0,
                      BsmtFinSF1 = 0,
                      BsmtFinSF2 = 0,
                      BsmtUnfSF = 0 ,
                      TotalBsmtSF = 0,
                      BsmtFullBath = 0,
                      BsmtHalfBath = 0))

    ## Add this bit for another set: LotFrontage = 0, if the above gives a poor result

##IMPUTE LOTFRONTAGE
## Predict the LotFrontage based on total sqft and Configuration of the lot with m.lf
data_ltz<-subset(data_f, is.na(LotFrontage)==FALSE)
m.lf <- glm(LotFrontage~LotArea+LotConfig, Gamma(link="log"), data = data_ltz)    
#summary(m.lf)
remove(data_ltz)
newdata<-data_f
newdata$LotFrontageHat<- predict(m.lf,newdata,type="response")  
iqr<-IQR(newdata$LotFrontage,na.rm=TRUE)
q3<-quantile(newdata$LotFrontage, probs=c(0.75), na.rm=TRUE)

#Cap it
newdata$LotFrontageHat<-case_when(
newdata$LotFrontageHat >= (q3+2.5*iqr) ~ (q3+2.5*iqr),                            
      TRUE ~newdata$LotFrontageHat
      )
remove(iqr)
remove(q3)
#Impute it
newdata$LotFrontageF<-case_when(
      is.na(newdata$LotFrontage)==TRUE ~ round(newdata$LotFrontageHat,0),
                      TRUE ~newdata$LotFrontage)
newdata$Cat<-case_when(
      is.na(newdata$LotFrontage)==TRUE ~ "Impute",
                      TRUE ~ "Original")

#Add it back to the dataset we are cleaning
data_f$LotFrontage<-newdata$LotFrontageF
data_f$LotFCat<-newdata$Cat
remove(newdata)


data_f<-subset(data_f,log(data_f$SalePrice)<=13.017 | is.na(data_f$SalePrice)==TRUE)


```

```{r REGROUPING}
##Regroup poorly represented factor levels
#Data_fg will be the set that is used for modelling

rm(data_fg)
data_fg<-data_f[,81:82]

#Resulting factor changes
data_fg$Era<-as.factor(case_when(data_f$MSSubClass == 20 ~ 'Newer',
                   data_f$MSSubClass == 60 ~ 'Newer',
                   data_f$MSSubClass == 120 ~ 'Newer',  
                   data_f$MSSubClass == 160 ~ 'Newer',
                     data_f$MSSubClass == 30 ~ 'Older',
                     data_f$MSSubClass == 70 ~ 'Older',
                   TRUE ~ 'NoAgeDifferentiation'  ))

data_fg$MSZoning<-as.factor(case_when(data_f$MSZoning ==  "C (all)"  ~ "C (all)",
                            data_f$MSZoning =="FV" ~ "FV",
                            data_f$MSZoning %in% c("RL","RP") ~ "RL/RP",
                            TRUE ~'Other'))

data_fg$MSZoning <-fct_reorder(data_fg$MSZoning, data_fg$SalePrice)

data_fg<-cbind(data_fg, data_f[,4:7])
data_fg$LotShape<-as.factor(case_when(data_f$LotShape == "Reg" ~ "Reg",
                            TRUE ~ "Irreg"))

data_fg$LandContour<- as.factor(case_when(data_f$LandContour == "HLS" ~ "Other",
                                          data_f$LandContour == "Low" ~ "Other",
                                          TRUE ~ data_f$LandContour))
#Discard Utilities
data_fg$LotConfig <- as.factor(data_f$LotConfig)
data_fg$LandSlope<-as.factor(case_when(data_f$LandSlope == "Gtl" ~ "Gtl",
                            TRUE ~ "Sloped"))
data_fg$Neighborhood <- as.factor(case_when(
  data_f$Neighborhood %in% c("IDOTRR", "BrDale") ~ "IDOTRR_BrDale",
  data_f$Neighborhood %in% c("Blueste", "Sawyer") ~ "Blueste_Sawyer",
  TRUE~data_f$Neighborhood) )
data_fg$Neighborhood <- fct_reorder(data_fg$Neighborhood,data_fg$SalePrice)

data_fg$Condition1 <-case_when(
  substring(data_f$Condition1,1,3) %in% c("Art", "Fee") ~ "ByRd",
  substring(data_f$Condition1,1,3) %in% c("RRA", "RRN") ~ "RR",
  TRUE ~ substring(data_f$Condition1,1,3))

data_fg$Condition2 <- case_when(
                                 data_f$Condition2 == "Norm" ~ "2Nor",
                                 substring(data_f$Condition2,1,3) == "Pos" ~ "3Pos",
                                 TRUE ~ "1Neg"
                                        )

#data_fg$ConComb<-paste(data_fg$Condition1 , data_fg$Condition2, sep = "_") 

data_fg$BldgType<-as.factor(data_f$BldgType)
data_fg$Stories<-as.factor(case_when(data_f$HouseStyle == "1Story" ~ "1.0",
                          data_f$HouseStyle == "1.5Fin" ~ "1.5", 
                          data_f$HouseStyle =="1.5Unf" ~ "1.5",
                          data_f$HouseStyle == "2Story" ~ "2.0",          
                          data_f$HouseStyle == "2.5Fin" ~ "2.5",
                          data_f$HouseStyle == "2.5Unf" ~ "2.5",  
                          TRUE ~ 'Other'  ))
data_fg<-cbind(data_fg, data_f[,18:19])
data_fg$YearBuilt<-data_f$YearBuilt
data_fg$AgeOfHse<-data_f$YrSold - data_f$YearBuilt

data_fg$YearRemodAdd<-case_when(data_f$YearRemodAdd<=data_f$YearBuilt ~ data_f$YearBuilt,
                                TRUE ~ data_f$YearRemodAdd)
data_fg$YrsSRemod<-data_f$YrSold - data_fg$YearRemodAdd
data_fg$RemodYN<- as.factor(case_when(data_fg$YearRemodAdd==data_fg$YearBuilt ~ "Y",
                            TRUE ~ "N"))
data_fg$RoofStyle<- as.factor(data_f$RoofStyle)
data_fg$RoofMatl<-as.factor(case_when(data_f$RoofMatl == "CompShg" ~ "CompShg",
                                      TRUE ~ "Other"))
#unique(data_fg$Exterior1st)
data_fg$Exterior1st<-as.factor(case_when(
data_f$Exterior1st %in% c("AsbShng", "AsphShn", "WdShing") ~"Shingles", 
data_f$Exterior1st %in% c("VinylSd", "MetalSd", "Wd Sdng") ~ "Siding",
data_f$Exterior1st %in% c("CemntBd", "Plywood", "HdBoard", "BrkFace" ) ~ "Board",
data_f$Exterior1st %in% c("BrkComm","CBlock", "ImStucc", "Stucco", 
                         "Other", "PreCast", "Stone") ~ "Other",
                    is.na(data_f$Exterior1st) ~ "Other",
                    TRUE ~ data_f$Exterior1st))

data_fg$Exterior2nd<-as.factor(case_when(
  data_f$Exterior2nd %in% c("AsbShng", "AsphShn", "WdShing", "Wd Shng") ~"Shingles",   data_f$Exterior2nd %in% c("VinylSd", "MetalSd", "Wd Sdng") ~ "Siding",
  data_f$Exterior2nd %in% c("CemntBd", "CmentBd", "Plywood", "HdBoard", "BrkFace" ) ~ "Board",
  data_f$Exterior2nd %in% c("BrkComm", "Brk Cmn", "CBlock", "ImStucc", "Stucco", 
                            "Other", "PreCast", "Stone") ~ "Other",
               is.na(data_f$Exterior2nd) ~ "Other",
               TRUE ~ data_f$Exterior2nd))
#data_fg$Ext_Concat<-as.factor(case_when(data_fg$Exterior1st == data_fg$Exterior2nd ~ 
 #                                         paste("Dbl_",data_fg$Exterior1st),
  #        TRUE ~ paste(data_fg$Exterior1st,'_Mixed')
   #       ))

data_fg$CntExtFin<- case_when(data_f$Exterior1st==data_f$Exterior2nd ~ 1 ,
                              TRUE ~ 2)

data_fg$MasVnrType<- as.factor(data_f$MasVnrType)
data_fg$MasVnrArea<- case_when((is.na(data_f$MasVnrArea)==TRUE & 
                                   data_f$MasVnrType =="None") ~ 0,
                               TRUE ~ data_f$MasVnrArea)

data_fg<-cbind(data_fg, data_f[,c("ExterQual","ExterCond")])

data_fg$Foundation<- as.factor(
  case_when(data_f$Foundation %in% c("BrkTil", "CBlock", "PConc") ~ data_f$Foundation,
  TRUE ~ "Other"
  ))
data_fg<-cbind(data_fg,data_f[,c(31:39, 48,49)])
data_fg$BsmtGenInfo<-case_when(data_fg$BsmtQual=="None" ~ "NoBsmt",
                        (data_fg$BsmtQual %in% c("Ex", "Gd", "TA") & 
                          data_fg$BsmtFullBath > 0 &   
                          (data_fg$BsmtFinType1 %in% c("GLQ", "ALQ") |
                               data_fg$BsmtFinType2 %in% c("GLQ","ALQ"))) ~ "RentBsmt",
                        (data_fg$BsmtQual %in% c("Ex", "Gd", "TA") & 
                          data_fg$BsmtCond %in% c("Ex", "Gd", "TA") &
                          (data_fg$BsmtHalfBath+ data_fg$BsmtFullBath) > 0)  ~ "SelfConRecS", 
                        (data_fg$BsmtFinType1 == "Rec" | data_fg$BsmtFinType2 == "Rec") ~ "RecSpace", 
                        (data_fg$BsmtQual %in% c("Ex", "Gd", "TA") & 
                          data_fg$BsmtCond %in% c("Ex", "Gd", "TA") &
                           (data_fg$BsmtFinType1 %in% c("GLQ", "ALQ") |
                               data_fg$BsmtFinType2 %in% c("GLQ","ALQ"))) ~ "RecSpace",  
                        data_fg$BsmtUnfSF == data_fg$TotalBsmtSF ~ "UnfOnly",
                        TRUE ~ "NWSO") #Needs work storage only

data_fg$BsmtGenInfo <-fct_reorder(data_fg$BsmtGenInfo, data_fg$SalePrice)

data_fg$CentralAir<-data_f$CentralAir
data_fg$Heating<-case_when(data_f$Heating == "GasA" ~ "GasA",
                           TRUE ~ "Other")
data_fg$HeatingQC<-as.factor(case_when(data_f$HeatingQC %in% c("Po", "Fa") ~ "PoFa",
                           TRUE ~ data_f$HeatingQC))
data_fg$HeatingQC<-fct_reorder(data_fg$HeatingQC, data_fg$SalePrice)

data_fg$Electrical <- case_when(data_f$Electrical %in% 
                                  c("SBrkr", "FuseA") ~ data_f$Electrical,
                                TRUE~"Other")
data_fg<-cbind(data_fg, data_f[,c(44:47,50:53,55)])

data_fg$GardenPerc<-(pmax(data_fg$X1stFlrSF, data_fg$X2ndFlrSF)/data_fg$LotArea)*100

data_fg$KitchenQual<-fct_reorder(as.factor(case_when(data_f$KitchenQual %in% c("Ex", "Gd", "TA") ~data_f$KitchenQual,
                               TRUE ~ "FaOrLess")),data_fg$SalePrice)
data_fg$Functional<-case_when(data_f$Functional =="Typ" ~ "Typ",
                               TRUE ~ "Dedctns")

data_fg<-cbind(data_fg, data_f[,(57:58)])

data_fg$GarageType<-case_when(is.na(data_f$GarageYrBlt)==TRUE ~ "None", 
                              TRUE ~ data_f$GarageType)
data_fg$GarageYrBlt<-case_when(data_f$GarageYrBlt > data_f$YrSold ~ data_f$YrSold,
                           is.na(data_f$GarageYrBlt) == TRUE ~ (year(now())+1),
                           TRUE ~data_f$GarageYrBlt)
data_fg<-cbind(data_fg, data_f[,61:72])
data_fg$PoolQC<-case_when(data_f$PoolQC == "None" ~ "None",
                          TRUE ~ "HasPool")

data_fg<-cbind(data_fg, data_f[,74:78])

data_fg$GarageQual<-case_when(data_fg$GarageQual %in% c('Ex', 'Gd') ~ 'ExGd',
                              data_fg$GarageQual %in% c('Po', 'None') ~ 'PoNone',
                              TRUE ~ data_fg$GarageQual )

data_fg$SaleType<-as.factor(case_when(data_f$SaleType == 'COD' ~ 'COD',
          data_f$SaleType == 'Con' ~ 'GCon', #Contract grouped
          data_f$SaleType == 'ConLw' ~ 'GCon',
          data_f$SaleType == 'ConLI' ~ 'GCon',
          data_f$SaleType == 'ConLD' ~ 'GCon',
          data_f$SaleType == 'New' ~ 'New',
          data_f$SaleType == 'WD' ~ 'GWD',
          data_f$SaleType == 'CWD' ~ 'GWD',
          data_f$SaleType == 'VWD' ~ 'GWD',
          TRUE ~'Other'
          ))
data_fg$SaleType<-(fct_reorder(data_fg$SaleType, data_fg$SalePrice))
data_fg$SaleCondition<-case_when(data_f$SaleCondition %in% c("Normal", "Partial") ~ data_f$SaleCondition , TRUE ~ "Other" )


```

# Post Cleaned, Grouped and Filled data:

Factors with no variance such as Utilities have been dropped

Other insights include the basement variables - can it be used for rental income?
```{r FixFactors}
chr <- sapply(data_fg, is.character)
colS<-names(data_fg[,chr])
data_fg[,colS]<-lapply(data_fg[,colS],factor)

remove(colS)               

plot_missing(data_fg)
```

The new plots show we have a complete data set. (Only the dependent variable for the test set is missing)

The data is split back into the training and test set, to start modelling. The training set is split into a validation set too, to test over-fitting since we are parameterising

The correlation heatmap from the ExploratoryData package's report is too dense to read properly. For a parameterised model multicollinearity is not good at all.We will split the dataset up into readable chunks and assess. 

```{r Resplit}
data_fg$Id<-data_f$Id
data_fg$LogY<-log(data_fg$SalePrice)

#test, train & val set
data_f_tst<-subset(data_fg, Type!="train_set")
data_f_tst<-data_f_tst[,-c(2)]
data_f_t<-subset(data_fg, Type=="train_set")  
data_f_t<-data_f_t[,-c(2)]

plot_correlation(data_f_t)

#Just for ease of reading
rm(test)
rm(train)
rm(data_f)
rm(m.lf)
```
###Correlation

To reduce the data set to a more manageable set for the parameterised models, lets remove the variables that have no correlation with log of SalePrice. This is done by first looking at the numerical variables and removing any where the p-value for the correlation between the ln of SalesPrice and the numeric variable is not significant (p >= 0.05) by Spearman correlation

The remaining variables are mostly assessed for near-perfect multicolinearity. 

```{r DropNum}
num<- sapply(data_f_t, is.numeric)
numS<-names(data_f_t[,num])
data_t_n<-data_f_t[,numS] # only numerical training data columns

#Find overall correlations for factor reduction between numericals

ovcor<- as.data.frame(round(cor(data.frame(lapply(data_t_n, as.integer)),
                                use = "pairwise.complete.obs",
                                method = "spearman"),2))
pcor<-as.data.frame(round(ggcorrplot::cor_pmat(ovcor),3))
cordep<-as.tibble(ovcor[,ncol(ovcor)]) # Last column has the log of Y
names(cordep) <- "cor_val"
pcordep<-as.tibble(pcor[,ncol(pcor)])
names(pcordep) <- "p_val"
cordep<-cbind(cordep, pcordep)
cordep<- cordep %>% mutate(cordep, Index = row_number(), Nom = colnames(data_t_n))

RemList<-subset(cordep, p_val >= 0.05 )[,4]
RemList<-RemList[-9]
cat('The following factors have low correlation with the dependent variable:', RemList)

#remove these values from the training sets
data_f_t<-dplyr::select(data_f_t, -(RemList))

k=ncol(data_f_t)
# Convert data to numeric
d<-data_f_t[,c(2:k)]
corr<- data.frame(lapply(d, as.integer))
#see correlelogram with numerical factors
ggcorr(corr,
    method = c("pairwise", "spearman"),
    nbreaks = 5,
    hjust = 1,
    size = 2,
    label = TRUE,
    label_size = 2,
    color = "grey50"
    )

        
rem2<-c("YearBuilt", "YearRemodAdd", 
        #"Condition1", "Condition2", "Exterior1st", "Exterior2nd", 
        "BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF",
        "X1stFlrSF", "X2ndFlrSF", "BsmtQual",
        "BsmtCond",  "BsmtFinType1", "BsmtFinType2")

##This was necessary when there was an overlap between the two sets. Now it breaks the model
#ii<-intersect(RemList, rem2)
#rem2<-rem2[rem2 != ii]

data_f_t<-dplyr::select (data_f_t ,-(rem2))

set.seed(22) 
sample = sample.split(data_f_t$SalePrice, SplitRatio = .85)
data_f_t<-dplyr::select(data_f_t, -LogY)

T_Train = subset(data_f_t, sample == TRUE)
T_Val  = subset(data_f_t, sample == FALSE)

rm(ovcor)
rm(pcor)
rm(pcordep)
rm(corr)
rm(d)
rm(data_t_n)
rm(rem2)
rm(num)
rm(numS)
rm(RemList)
```

The result is that we remove the following variables;

- YearBuilt
- YearRemodAdd

And secondly the following are linear combinations of other variables
- BsmtFinSF1
- BsmtFinSF2
- BsmtUnfSF for Total Basement SF

And GrLivArea is the sum of these
- X1stFlrSF 
- X2ndFlrSF

BsmtGenInfo is a composite of
- BsmtQual
- BsmtCond
- BsmtFinType1
- BsmtFinType2

### To remove here - SOM 

The natural log transform does seem to make the dependent variable more symmetrical. Lets explore the three GLMs (Gamma:log, Gamma:inv, gamma:id) with a forward stepwise variable selection to decide which GLM is best

In terms of the model selection we need a good balance between Mcfadden's pseudo R-square, Nagelkerke's R-square and plain R-square. As expected the McF's R2 is severely penalised due to too many parameters.


```{r DependentDistributionExploration}

ggplot(data_f_t, aes(x=SalePrice))+
  geom_density(aes(x=SalePrice, y=..density..))+
  ggtitle ("SalePrice Distribution")

ggplot(data_f_t)+
  geom_density(aes(x=log(SalePrice), y=..density..))+
  ggtitle("Distribution of natural logarithm of SalePrice")

```
### GLM Selections

A multi-loop was coded for GLM Stepwise Factor Selection which terminates when the model's AIC stops dropping. 

```{r GLM_Gamma_log}

results<-data.frame("int"=0, "factor_in" = 'var', "AIC" = 0.00, "R2" = 0.00)
keep<-data.frame("VarCnt"=0, "Exp"='exp', "int"=0, "factor_in" = 'var', "AIC" = 0.00, "R2" = 0.00)
t=ncol(T_Train)-1
unk_r2=data.frame("new_r2"=0)
cond <- 0

#j=2
for (j in 1:t){
  if (j < 2){
    y="SalePrice~"
    y_exp<-y
    X_vars<-T_Train
  } else {
    y_exp<-paste(y_exp, keep[(j-1),4],'+')
    }
  if (j>1 & cond > 0){
    break
  }
  k=ncol(X_vars)
 # print(k)
 # print(y_exp)
  for (i in 1:(k-2)) {
    #for (i in 1: 11) {
    #x=names(T_Train)[(i+1)]
    var = names(X_vars)[i+1]
    mod<-glm(paste(y_exp,var),
             family = Gamma(link="log"), 
             data = T_Train)
    AIC<-as.numeric(mod$aic)
    R2<-as.numeric(with(summary(mod), 1 - deviance/null.deviance))
    ires<-cbind(i,var,AIC,R2)
    results[i,]<-ires
  }
  
  keep[j,]<-cbind(j, y_exp, results[order(results$R2, decreasing = TRUE), ][1,])
  use_mod_gamlog<-glm(paste(keep[j,2],keep[j,4]),
               family = Gamma(link="log"), 
               data = T_Train)
  AP<-predict(object = use_mod_gamlog, newdata = T_Val, type = "response")
  unk_r2[j,1]<-cor(AP,T_Val[,1], method = "spearman")
  X_vars<-dplyr::select(X_vars,-(keep[j,4]))
  if (j>1) {
    cond <- as.numeric(keep[j,5]) - as.numeric(keep[j-1,5])
    } else { cond<- 0 }
}

ResultSummary<-cbind(keep,unk_r2)
ResultSummary<-ResultSummary[1:nrow(ResultSummary)-1,]
LastExp<-paste(ResultSummary[nrow(ResultSummary),2],ResultSummary[nrow(ResultSummary),4])
ResultSummary<-dplyr::select(ResultSummary, -"Exp")
ResultSummary

BestModUnrfn<-glm(LastExp,
               family = Gamma(link="log"), 
               data = T_Train)

summary(BestModUnrfn)

#T_Val$yh<-exp(predict(BestModUnrfn, T_Val))
#ggplot(data=T_Val, aes(x=SalePrice, y = yh))+ geom_jitter()+geom_line(aes(x=SalePrice, y = #SalePrice))
#hist(data_fg$LotFrontage)
#LotFrontage above 150feet seems unrealistic. It also causes a major outlier. Should cap the prediction to the 5% percentile of the neighbourhood.... 

rm(cordep)
rm(ires)
rm(keep)
rm(results)
rm(ii)
rm(j)
rm(X_vars)
rm(use_mod_gamlog)
```



```{r RefineGamLog}
ggplot(ResultSummary, aes(x=VarCnt) )+
  geom_line(aes(y=new_r2), color = "red")+
  geom_line(aes(y=as.numeric(R2)), color = "blue")

ggplot(ResultSummary, aes(x=VarCnt, y=as.numeric(AIC)) )+
  geom_line()

set.seed(22) 
sample = sample.split(data_f_t$SalePrice, SplitRatio = .85)

##FIX DATA GROUPS FOR PARAM SIGNIF
L<-c(char(ResultSummary$factor_in), "Id")
GamData<-dplyr::select(data_f_t, L)
GamData2<-dplyr::select(data_f_tst, L)
GamData2<-cbind("SalePrice" = NA, GamData2)
GamData2$Grp<-"Test"

GamData<-cbind("SalePrice"=data_f_t[,1],GamData)
GamData$Grp<-case_when(sample == TRUE ~ "Train", TRUE~ "Val")
#Group the test and training sets together to wrangle the data once.                         
GamData<-rbind(GamData,GamData2)                         

GamData$BsmtGenInfo<-case_when(
  GamData$BsmtGenInfo == "RentBsmt" ~ GamData$BsmtGenInfo,
  TRUE~"Other")
GamData$FireplaceQu<-case_when(
  GamData$FireplaceQu == "None" ~ "None",
  TRUE~"HasFP")
GamData$BldgType <-fct_reorder(case_when(
  GamData$BldgType %in% c("1Fam", "2fmCon","Duplex") ~ "1_2fm_Dup",
                                    TRUE ~ GamData$BldgType), GamData$SalePrice)
GamData$Condition1<- case_when(
  GamData$Condition1 %in% c("ByRd", "RR") ~ "ByRdRR",
  GamData$Condition1 %in% c("Nor", "Pos") ~ "NorPos",
  TRUE ~ "error"  )
GamData$BsmtExposure<-case_when(
  GamData$BsmtExposure =="Gd" ~ "Gd",
  TRUE~"Other")
GamData$BsmtExposure<-fct_reorder(GamData$BsmtExposure, GamData$SalePrice)

# GamData$GarageType<-fct_reorder(case_when(
#   GamData$GarageType %in% c("CarPort","None", "2Types") ~ "None_Cprt",
#   GamData$GarageType %in% c("Attchd","Basment", "BuiltIn") ~ "PlanAtt",
#   TRUE~GamData$GarageType), GamData$SalePrice)


GamData$HeatingQC <-fct_reorder(case_when(
  GamData$HeatingQC %in% c("Ex","Gd") ~ "ExGd",
                                    TRUE ~ "PoFaTA"),GamData$SalePrice)
GamData$KitchenQual <-fct_reorder(case_when(GamData$KitchenQual == "Ex" ~ "Ex",
                                 TRUE ~ "NormnLess"),GamData$SalePrice)

GamData$GarageQual<-fct_reorder(case_when(
  GamData$GarageQual %in% c("None", "Po") ~ "NonePoor",
  GamData$GarageQual %in% c("TA","Fa") ~ "FairTyp",
                                    TRUE ~ "ExGd"),GamData$SalePrice)

# GamData$SaleType <-fct_reorder(case_when(
#   GamData$SaleType == "New" ~ GamData$SaleType,
#  TRUE ~ "Other"),GamData$SalePrice)

GamData$LotConfig <-fct_reorder(case_when(
  GamData$LotConfig %in% c( "CulDSac") ~ GamData$LotConfig,
 TRUE ~ "NotCDSac"),GamData$SalePrice)

GamData$Foundation <-fct_reorder(case_when(
  GamData$Foundation == "PConc" ~ "PConc",
  TRUE ~ "Other"), GamData$SalePrice)

GamData$Condition2 <-fct_reorder(case_when(
  GamData$Condition2 == "3Pos" ~ "Pos",
  TRUE ~ "NegNor"), GamData$SalePrice)

chr2 <- sapply(GamData, is.character)
colS<-names(GamData[,chr2])
GamData[,colS]<-lapply(GamData[,colS],factor)
remove(colS)  
remove(chr2)

GamT_Train = subset(GamData,Grp == "Train")
GamT_Val  = subset(GamData, Grp == "Val")
GamT_Score  = subset(GamData, Grp == "Test")

RefGamGLM<-glm(LastExp,
               family = Gamma(link="log"), 
               data = GamT_Train)

#This is the version with outliers
#SalePrice~ OverallQual + GrLivArea + Neighborhood + BsmtGenInfo + GarageArea + OverallCond + TotalBsmtSF + AgeOfHse + BldgType + MSZoning + Fireplaces + BsmtExposure + KitchenQual + Functional + Condition1 + HeatingQC + GarageQual + SaleType + LotArea + Condition2 + ExterCond + CentralAir + YrsSRemod + LotConfig + Foundation"

# This is the eq without the outliers - should really remove the var that has no signifincat levels...(GarageQual)

#"SalePrice~ OverallQual + Neighborhood + GrLivArea + BsmtGenInfo + GarageArea + OverallCond + TotalBsmtSF + AgeOfHse + FireplaceQu + MSZoning + BldgType + LotArea + SaleCondition + Condition1 + Functional + BsmtExposure + KitchenQual + CentralAir + Condition2 + GarageQual + YrsSRemod + ExterCond + LotConfig + HeatingQC + Foundation + WoodDeckSF"

summary(RefGamGLM)

as.numeric(with(summary(RefGamGLM), 1 - deviance/null.deviance))
APR<-predict(object = RefGamGLM, newdata = GamT_Val, type = "response")
cor(APR,GamT_Val[,1], method = "spearman")

t<-as.data.frame(cbind("Actual"=GamT_Val[,1],"Predicted"=APR, "Neighborhood"=GamT_Val[,3]))
t$Neighborhood<-as.character(GamT_Val[,3])
t$LotArea<-GamT_Val$LotArea
tt<-rbind(melt(t, id=3, measure = 1),melt(t, id=3, measure = 2))


ggplot(data=t, aes(x=Actual, y = Predicted))+ geom_jitter(color = "red")+ geom_line(aes(x=Actual, y=Actual), colour = "green")

ggplot(tt, aes(x=Neighborhood))+geom_boxplot(aes(y=value, fill = variable))+
    theme(axis.text.x = element_text(angle=90))+
 ggtitle("Identify Outliers for capping - Idea1")

ggplot(t, aes(x=LotArea))+geom_jitter(aes(y=Actual), color = "purple")+geom_jitter(aes(y=Predicted), color = "pink")+
    theme(axis.text.x = element_text(angle=90))+
 ggtitle("Identify Outliers for capping - Idea2")

CapPred<- t %>% 
  group_by(Neighborhood) %>%  
  summarise(P99Actual = quantile(Actual, c( 0.99)))
CapPred$P99Actual<-as.numeric(CapPred$P99Actual)

ggplot(data=CapPred, aes(x=Neighborhood, y = P99Actual))+ geom_jitter()+
  theme(axis.text.x = element_text(angle=90))+
 ggtitle("Capping outliers by Neighbourhood at 99th Percentile")


```

```{r Prediction}
GamT_Score$Upload_Gam4<-predict(object = RefGamGLM, newdata = GamT_Score, type = "response")

GamT_Score <- GamT_Score %>% 
  left_join(CapPred, by = c("Neighborhood")) 

GamT_Score$SalePrice<-case_when(GamT_Score$Upload_Gam4>GamT_Score$P99Actual ~ GamT_Score$P99Actual,
                                TRUE~GamT_Score$Upload_Gam4)
GamT_Score$SalePriceOR<-case_when(GamT_Score$Upload_Gam4>GamT_Score$P99Actual ~ "Y",
                                TRUE~"N")

ggplot(GamT_Score, aes(x=SalePriceOR)) + geom_bar()

ggplot(GamT_Score)+geom_jitter( aes(x=Upload_Gam4 , y=P99Actual, color=SalePriceOR))

#Capped set
Sub1_NoOutGamGLM<-GamT_Score[,c("Id","SalePrice")]
write.csv(Sub1_NoOutGamGLM, "Sub3_NoOutGamGLM.csv")

#Extreme OL Fixed
Sub2_GamGLM<-GamT_Score[,c("Id","Upload_Gam4")]
Sub2_GamGLM$Upload_Gam4<-case_when(GamT_Score$Upload_Gam4>500000 ~ 500000, 
                                   TRUE ~  GamT_Score$Upload_Gam4)
write.csv(Sub2_GamGLM, "Sub4_GamGLM.csv")

```

```{r TradSWLM}
LogY<-log(T_Train$SalePrice)
T_TrainLY<-cbind(LogY, T_Train[,2:ncol(T_Train)])

full.model <- lm(LogY~., data = T_TrainLY)
step.LY.model <- stepAIC(full.model, direction = "both", trace = FALSE)
summary(step.LY.model)

FittedValues<-step.LY.model$fitted.values

#Most likely completely overfitted
ValPredLY<-data.frame(predict(object=step.LY.model, newdata=T_Val,type = "response"))
names(ValPredLY)<-"FittedValues"
ValPred<-exp(ValPredLY)
cor(ValPred,T_Val$SalePrice)

data_f_tst<-subset(data_fg, Type!="train_set")


LM_Pred_Train<-exp(rbind(data.frame(FittedValues),ValPredLY))


LMPredLY<-predict(object=step.LY.model, newdata=data_f_tst, type = "response")
LMPred<-exp(LMPredLY)
LMUPload<-data.frame(cbind("Id"=data_f_tst$Id, "SalePrice" = LMPred))
write.csv(LMUPload, "LM_Stepwise_tradLogY.csv", row.names = F)
```

After that we will do some feature engineering to see if there is an improvement

These are unused codes

Lasso regression is a technique that happily accommodates multi-colinear data so we can work on the data set called data_fg

```{r LASSO}
library(glmnet)
Lasso_Train = subset(data_fg, Type == "train_set")
Lasso_Test  = subset(data_fg, Type == "test_set")

LassoUpload<-data.frame("Id"= Lasso_Test$Id)

#Remove the Y's for the matrix creation 
Lasso_Test <- Lasso_Test[,2:(ncol(Lasso_Test)-1)]

# START BY PULLING ALL DATA AS PER AN X MATRIX; Categorical data is dummy coded (one hot encoding)
LTN_X <- model.matrix(SalePrice ~ . - 1 - Id - LogY - Type, data = Lasso_Train)
LTT_X <- model.matrix(~ . - 1 - Id - Type , data = Lasso_Test)

LTN_Y <- Lasso_Train$SalePrice

# Define the cross-validation for lambda selection
cv_nonstd <- cv.glmnet(LTN_X, LTN_Y, alpha = 1, nfolds = 10)

plot(cv_nonstd) 

# Fit the Lasso model with the best lambda
best_lambda <- cv_nonstd$lambda.min

lasso_model <- glmnet(LTN_X, LTN_Y, alpha = 1, lambda = best_lambda)

coef(lasso_model)

fitted_train<-predict(lasso_model, s = best_lambda, newx = LTN_X)
fitted_test<-predict(lasso_model, s = best_lambda, newx = LTT_X)

#Goodnessof Fit
#No std
cor(fitted_train,LTN_Y, method = "spearman")
sst <- sum((LTN_Y - mean(LTN_Y))^2)
sse <- sum((fitted_train - LTN_Y)^2)
rsq <- 1 - sse/sst
rsq

lassoplot<-data.frame(fitted_train,"Actual"=LTN_Y)
names(lassoplot)[1]<-"Fitted"

ggplot(data = lassoplot, aes(x=Fitted, y = Actual))+
  geom_jitter()+
  ggtitle("Non Standardised Lasso Model Actual vs Fitted")

LassoUpload$SalePrice<-fitted_test
names(LassoUpload)[2]<-"SalePrice"
str(LassoUpload)

write.csv(LassoUpload, "Lasso.csv", row.names = FALSE)

```

```{r RANDOMFOREST}
library(randomForest)
library(caret)
library(ranger)
#Establish baselines
data_fg<-dplyr::select(data_fg, -"LogY")
RF_Train = subset(data_fg, Type == "train_set")
RF_Train <-RF_Train[,c(1,3:(ncol(RF_Train)-1))]
RF_Test  = subset(data_fg, Type == "test_set") # has two more cols- type and id

#Xcol<-ncol(LTN_X) # Set for Lasso model
#What is the best tree number
s<-1983
set.seed(s)
mod1 <- randomForest(
  formula = SalePrice ~ .,
  data    = RF_Train,
  ntree   = 2000
)
mod1
mod1$mtry # 28
plot(mod1)

which.min(mod1$mse) # Different every time; 1234 when ntree = 2000
# RMSE of this optimal random forest
sqrt(mod1$mse[which.min(mod1$mse)]) # $23K
#mod1$importance
varImpPlot(mod1) # Super similar to the choice of addition & order of the gamma GLM

rm(mod2)
#Search & Tune
mod2 <- tuneRF(
  x          = RF_Train[,2:ncol(RF_Train)],
  y          = RF_Train$SalePrice,
  ntreeTry   = 600, # Previously set this to 2000, gave value of 33
  mtryStart  = 5,
  stepFactor = 1.5,
  improve    = 0.01,
  trace      = FALSE      # to not show real-time progress 
)
plot(mod2) # Shows the mtry optimum is at 22

# Lets use train
Cn <- trainControl(method="repeatedcv", number=5, repeats=3) 
#metric <- "Accuracy"
set.seed(s)
tunegrid <- expand.grid(.mtry=22)
RF_trainpackdef <- train(SalePrice~., data=RF_Train, method="rf",  tuneGrid=tunegrid, trControl=Cn)
print(RF_trainpackdef)

# Random Search
Rcontrol <- trainControl(method="repeatedcv", number=10, repeats=3, search="random")
rf_trainpackrandom <- train(SalePrice~., data=RF_Train, method="rf", tuneLength=5, trControl=Rcontrol)
print(rf_trainpackrandom)
plot(rf_trainpackrandom)

rm(hyper_grid)
# To reduce the search - start by checking the sample size; set to .75 & default
hyper_grid <- expand.grid(
  mtry       = seq(20, 34, by = 2),
  node_size  = seq(3, 15, by = 2),
  sampe_size = c(.632, .75, 0.8),
  OOB_RMSE   = 0
)

# total number of combinations
nrow(hyper_grid)
## 168


for(i in 1:nrow(hyper_grid)) {
  
  # train model with full grid search
  gs_model <- ranger(
    formula         = SalePrice ~ ., 
    data            = RF_Train, 
    num.trees       = 600,
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$node_size[i],
    sample.fraction = hyper_grid$sampe_size[i],
    seed            = s
  )
  
  # add OOB error to grid
  hyper_grid$OOB_RMSE[i] <- sqrt(gs_model$prediction.error)
}

hyper_grid_res<-hyper_grid %>% 
  dplyr::arrange(OOB_RMSE) %>%
  head(20)

hyper_grid_res

optimal_ranger <- ranger(
    formula         = SalePrice ~ ., 
    data            = RF_Train, 
    num.trees       = 600,
    mtry            = 32,
    min.node.size   = 3,
    sample.fraction = .8,
    importance      = 'impurity'
  )
  
optimal_ranger$variable.importance %>% 
  broom::tidy() %>%
  dplyr::arrange(desc(x)) %>%
  dplyr::top_n(25) %>%
  ggplot(aes(reorder(names, x), x)) +
  geom_col() +
  coord_flip() +
  ggtitle("Top 25 important variables")

##SET UP THE PREDICTIONS




```

```{r UNIFYTRAIN}
AllTrain<-subset(data_fg,Type=="train_set")
AllTrain<-dplyr::select(AllTrain, -"LogY")
AllTrain<-AllTrain[,c(87,2:86,1)]
AllTrain$Lasso<-lassoplot$Fitted
glmyh<-predict(object = RefGamGLM, newdata = subset(GamData, Grp != "Test"), type = "response")
AllTrain$BestGLM<-glmyh
AllTrain$TradLogYtfm<-LM_Pred_Train$FittedValues


ggplot(AllTrain)+geom_smooth(aes(x=SalePrice, y=BestGLM), colour = "red")+
geom_smooth(aes(x=SalePrice, y=Lasso), colour = "blue")+
geom_smooth(aes(x=SalePrice, y=TradLogYtfm), colour = "yellow")+
  geom_smooth(aes(x=SalePrice, y = SalePrice), colour="black")

```

```{r UNIFYVAL}

AllTest<-LassoUpload
names(AllTest)[2]<-"Lasso"
AllTest$BestGLM<-Sub2_GamGLM$Upload_Gam4
AllTest$TradLogYLM<-LMUPload$SalePrice
names(AllTest)

ggplot(AllTest)+geom_jitter(aes(x=Id, y=BestGLM), colour = "red")+
geom_jitter(aes(x=Id, y=Lasso), colour = "blue")+
geom_jitter(aes(x=Id, y=TradLogYLM), colour = "yellow")
head(AllTest)

```



```{r UNIFY}

AllTest<-LassoUpload
names(AllTest)[2]<-"Lasso"
AllTest$BestGLM<-Sub2_GamGLM$Upload_Gam4
AllTest$TradLogYLM<-LMUPload$SalePrice
names(AllTest)

ggplot(AllTest)+geom_jitter(aes(x=Id, y=BestGLM), colour = "red")+
geom_jitter(aes(x=Id, y=Lasso), colour = "blue")+
geom_jitter(aes(x=Id, y=TradLogYLM), colour = "yellow")
head(AllTest)

```



